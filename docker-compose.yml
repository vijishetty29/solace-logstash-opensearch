services:
  # 1. Solace PubSub+ Broker
  solace:
    image: solace/solace-pubsub-standard:latest
    container_name: solace
    privileged: true
    shm_size: 2g
    ulimits:
      nofile:
        soft: 2448
        hard: 4096
      nproc:
        soft: 2048
        hard: 2048
    ports:
      - "8080:8080"
      - "5672:5672"
      - "55554:55555" # <-- ADD THIS LINE for SMF
    environment:
      - username_admin_globalaccesslevel=admin
      - username_admin_password=admin
      - system_scaling_maxconnectioncount=100
    volumes:
      - solace_storage:/usr/solace/storage
    # --- ADD THIS ENTIRE 'healthcheck' BLOCK ---
    # THIS HEALTHCHECK IS CRITICAL
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl -f -u admin:admin http://localhost:8080/SEMP/v2/monitor/about || exit 1",
        ]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s
    networks:
      - app-net

  opensearch:
    image: opensearchproject/opensearch:2.11.0 # You can use a more recent version if available
    container_name: opensearch
    environment:
      - cluster.name=opensearch-cluster
      - node.name=opensearch-node
      - discovery.type=single-node
      - bootstrap.memory_lock=true # along with the memlock settings in a moment
      - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m" # minimum and maximum Java heap size, recommend setting to 50% of server RAM
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536 # Maximum number of open files
        hard: 65536
    volumes:
      - opensearch_data:/usr/share/opensearch/data
    healthcheck:
      test: [
          "CMD-SHELL",
          # Updated to use HTTPS, ignore self-signed cert (-k), and use auth (-u)
          "curl -f -k -u admin:admin https://localhost:9200/_cluster/health?wait_for_status=yellow&timeout=1s || exit 1",
        ]
      interval: 10s
      timeout: 5s
      retries: 5
    ports:
      - 9200:9200
      - 9600:9600 # required for Performance Analyzer
    networks:
      - app-net
  opensearch-dashboards:
    image: opensearchproject/opensearch-dashboards:2.11.0 # Use the same version as opensearch-node
    container_name: opensearch-dashboards
    ports:
      - 5601:5601
    expose:
      - "5601"
    environment:
      OPENSEARCH_HOSTS: '["https://opensearch:9200"]'
      # DISABLE_SECURITY_DASHBOARDS_PLUGIN=true is not needed if the main plugin is disabled
      OPENSEARCH_SECURITY_USERNAME: admin
      OPENSEARCH_SECURITY_PASSWORD: admin
      OPENSEARCH_SSL_VERIFICATIONMODE: none
      NODE_TLS_REJECT_UNAUTHORIZED: 0

    networks:
      - app-net

  # 3. Logstash Processor
  logstash:
    build:
      context: ./logstash-docker # <-- Builds custom Dockerfile
    container_name: logstash
    ports:
      - "5044:5044"
    volumes:
      - ./pipeline:/usr/share/logstash/pipeline/
    environment:
      - "LS_JAVA_OPTS=-Xms256m -Xmx256m"
    depends_on:
      opensearch:
        condition: service_healthy
    networks: # <-- THIS IS CRITICAL
      - app-net

  # 4. Go AMQP Connector
  go-connector:
    container_name: go-connector
    build:
      context: ./go-connector
    environment:
      - SOLACE_HOST=solace
      - SOLACE_USER=admin
      - SOLACE_PASS=admin
      - LOGSTASH_TCP_HOST=logstash:5044
      - SOLACE_TOPIC=logs/data/>
      - SOLACE_QUEUE=logstash_ingest_queue
    # --- UPDATE THE 'depends_on' BLOCK ---
    depends_on:
      solace:
        condition: service_healthy # Wait for the healthcheck
      logstash:
        condition: service_started # Just wait for logstash to start
    restart: on-failure
    networks:
      - app-net

volumes:
  solace_storage:
  opensearch_data:

# Define the custom network
networks: # <-- THIS BLOCK IS CRITICAL
  app-net:
    driver: bridge
